# 项目简介

本项目以亚马逊电影评论数据集作为数据基础，针对电影及其周边信息，建立基于关系型数据库（MySQL）、分布式文件型数据仓库系统（Hadoop+Hive+Spark），和图数据库（Neo4j）的数据仓库系统，构建数据治理体系，并针对特定业务需求优化存储模型，定制化数据聚合，进而实现了高效率的综合条件查询，合作关系查询和数据统计，系统性能对比等业务功能。

# 功能列表

1. 查询电影信息

   - 提供的筛选条件字段：

   | 电影标题 | title                             |
   | -------- | --------------------------------- |
   | 上映日期 | year、month、day、season、weekday |
   | 导演     | director                          |
   | 演员     | actor                             |
   | 风格     | genre_name                        |
   | 评分     | min_score、max_score              |

   上述字段参数都**可空**，若空则表示不作为筛选条件。

   - 可选的电影信息字段：

   | 电影asin   | asin       |
   | ---------- | ---------- |
   | 电影标题   | title      |
   | 电影版本数 | edition    |
   | 电影版本   | format     |
   | 电影风格   | genre_name |
   | 电影评分   | score      |
   | 上映日期   | date       |
   | 导演       | directors  |
   | 演员       | actors     |

   上述字段都可选，并且若某一部电影有多个字段信息（如演员）则返回列表。

通过该综合查询的接口，我们实现了根据多重字段筛选电影、并返回电影不同信息的接口。

2. 查询合作关系

   - 查询与演员经常合作的导演
   - 查询与导演经常合作的演员
   - 查询与演员经常合作的演员

   | source | actor或director |
   | ------ | --------------- |
   | target | actor或director |
   | name   | 人名            |

# 运行示例

1. 综合查询电影信息

- 查询电影《Harry Potter & Chamber Of Secrets》的所有信息：

<img src="数据仓库项目报告.assets/(null)-20221230114830693.(null)" alt="img" style="zoom:50%;" />

查询结果：

<img src="数据仓库项目报告.assets/(null)-20221230114830691.(null)" alt="img" style="zoom:50%;" />

<img src="数据仓库项目报告.assets/(null)-20221230114830781.(null)" alt="img" style="zoom:50%;" />

速度对比：

<img src="数据仓库项目报告.assets/(null)-20221230114830678.(null)" alt="img" style="zoom:50%;" />

- 查询1月1日上映的，评分高于4分的Adventure类电影的标题、上映日期、评分、导演：

<img src="数据仓库项目报告.assets/(null)-20221230114830747.(null)" alt="img" style="zoom:33%;" />

查询结果：

<img src="数据仓库项目报告.assets/(null)" alt="img" style="zoom:50%;" />

速度对比：

<img src="数据仓库项目报告.assets/(null)-20221230114830791.(null)" alt="img" style="zoom:50%;" />

- 查询由Douglas Schwartz执导，David Charvet出演的评分高于3分的电影的标题、编号、评分、日期、导演、演员：

<img src="数据仓库项目报告.assets/(null)-20221230114830818.(null)" alt="img" style="zoom:50%;" />

查询结果：

<img src="数据仓库项目报告.assets/(null)-20221230114830795.(null)" alt="img" style="zoom:50%;" />

速度对比：

<img src="数据仓库项目报告.assets/(null)-20221230114830844.(null)" alt="img" style="zoom:50%;" />

1. 查询合作关系

- 查询与演员Chris Rock合作次数大于等于4次的演员：

<img src="数据仓库项目报告.assets/(null)-20221230114830885.(null)" alt="img" style="zoom:50%;" />

查询结果：

<img src="数据仓库项目报告.assets/(null)-20221230114830987.(null)" alt="img" style="zoom:50%;" />

速度对比：

<img src="数据仓库项目报告.assets/(null)-20221230114831088.(null)" alt="img" style="zoom:50%;" />

- 查询与导演Eric Darnell合作次数大于等于2次的演员：

<img src="数据仓库项目报告.assets/(null)-20221230114830897.(null)" alt="img" style="zoom:50%;" />

查询结果：

<img src="https://zperabw7qd.feishu.cn/space/api/box/stream/download/asynccode/?code=OWZkMzliYTI3ZjRlYWQzYTNhZDk0NmFmZjJiZjI0Y2NfQ3NiR1h4VmRidXBFd0JBd2g3VTVmVkZUTWpTaVlOTkdfVG9rZW46Ym94Y25kN3U4blJFRjZjY2R4d051TVRyMUhkXzE2NzIzNzIxMDA6MTY3MjM3NTcwMF9WNA" alt="img" style="zoom:50%;" />

速度对比：

<img src="数据仓库项目报告.assets/(null)-20221230114831006.(null)" alt="img" style="zoom:50%;" />

# ETL&数据清洗

## 获取用户评价

首先打开提供的[snap网页](http://snap.stanford.edu/data/web-Movies.html)，可发现它提供了一个下载链接，用于下载由约90万用户发出的与25万产品相关的近800万条评论。

<img src="数据仓库项目报告.assets/(null)-20221230114831059.(null)" alt="img" style="zoom:50%;" />

下载压缩包后解压，得到一个约9G的`movies.txt`文件。文件文本格式如下所示，其中productId即为每个产品对应的亚马逊产品号（ASIN），并且都是唯一的。

![img](数据仓库项目报告.assets/(null)-20221230114831151.(null))

直接用Python逐行读取，提取内容。具体代码见`extract_movies_txt.ipynb`

```Python
import pandas as pd

i = 0

productId = []
userId = []
profileName = []
helpfulness = []
score = []
time = []
summary = []
text = []

for line in open("../Data/Raw/movies.txt", 'r', encoding='UTF-8', errors='ignore'): 
    split = line.split(' ', 1)
#     print(split)
    if split == []:
        continue
        
    if split[0] == "product/productId:":
        productId.append(split[1])
    elif split[0] == "review/userId:":
        userId.append(split[1])
    elif split[0] == "review/profileName:":
        profileName.append(split[1])
    elif split[0] == "review/helpfulness:":
        helpfulness.append(split[1])
    elif split[0] == "review/score:":
        score.append(split[1])
    elif split[0] == "review/time:":
        time.append(split[1])
    elif split[0] == "review/summary:":
        summary.append(split[1])
    elif split[0] == "review/text:":
        text.append(split[1])
        i += 1
    
    if i % 1000000 == 0:
        print(i)
            
print(i)
# 字典中的key值即为csv中列名
dataframe = pd.DataFrame({'productId':productId,'userId':userId,'profileName':profileName,'helpfulness':helpfulness,'score':score,'time':time,'summary':summary,'text':text})

# 将DataFrame存储为csv,index表示是否显示行名，default=True
dataframe.to_csv("comments.csv",index=False,sep=',')
```

可以看到，提取过程中并没有对换行做出很好的处理，因此用pandas导入后再进行处理，去除换行（`get_date_from_comments.ipynb`）

```Python
comments['productId'] = comments['productId'].str.strip()
comments['userId'] = comments['userId'].str.strip()
comments['profileName'] = comments['profileName'].str.strip()
comments['helpfulness'] = comments['helpfulness'].str.strip()
comments['summary'] = comments['summary'].str.strip()
```

![img](数据仓库项目报告.assets/(null)-20221230114831066.(null))

## 爬取网页

### 获取ASIN

由官网介绍可得，每件产品对应的网址，其实就是`amazon.com/dp/$(ASIN)`，其中ASIN为对应的产品编号

![img](数据仓库项目报告.assets/(null)-20221230114831150.(null))

为此，首先在`movies.txt`中提取ASIN（`extract_movies_txt.ipynb`），并保存为`productId.csv`

```Python
import pandas as pd

productId = []
for line in open("../Data/Raw/movies.txt", 'r', encoding='UTF-8', errors='ignore'): 
    split = line.split()
#     print(split)
    if split == []:
        continue
    if split[0] == "product/productId:":
        if split[1] not in productId:
            productId.append(split[1])
#             print(productId)
     
# 字典中的key值即为csv中列名
dataframe = pd.DataFrame({'productId':productId})

# 将DataFrame存储为csv,index表示是否显示行名，default=True
dataframe.to_csv("productId.csv",index=False,sep=',')
```

### 定位网页内容

接下来对网页内容进行爬取。我们使用Scrapy＋Selenium进行爬取，具体步骤如下

首先安装Scrapy，再使用scrapy自带的命令行查看和分析单个网页

https://www.amazon.com/dp/B00006HAXW/

```Python
scrapy shell 'https://www.amazon.com/dp/B00006HAXW/'
```

之后可以在命令行中通过`view(response)`方法查看页面，也可以通过`response.xpath('...').get()`的方法获取数据。（xpath是一种XML路径语言，相比css更容易确定HTML页面中的位置）

xpath的语法不难，但是浏览器自带的开发者工具提供了更为方便的方式。在网页内容中右键选中对应的标签块（例如，`<span>`，`<h1`），右键即可复制其对应的XPath，并查看相关内容

![img](数据仓库项目报告.assets/(null)-20221230114831297.(null))

经分析，亚马逊的电影网页分成正常和Prime Video两种

正常页面如下

![img](数据仓库项目报告.assets/(null)-20221230114831248.(null))

我们选择爬取了该类型网页的Product details内容

<img src="数据仓库项目报告.assets/(null)-20221230114831277.(null)" alt="img" style="zoom:67%;" />

Prime Video的页面如下

![img](数据仓库项目报告.assets/(null)-20221230114831573.(null))

主要选取了以下部分内容

<img src="数据仓库项目报告.assets/(null)-20221230114831299.(null)" alt="img" style="zoom:50%;" /><img src="数据仓库项目报告.assets/(null)-20221230114831471.(null)" alt="img" style="zoom:50%;" />

### 爬取网页及数据

使用scrapy创建项目

```Python
scrapy startproject FilmInfoSpider
```

在`FilmInfoSpider/FilmInfoSpider`下建立py文件，并写入爬虫脚本，爬取的链接为上文中提取的25万个产品对应的网址

核心部分代码如下：

```Python
    def parse(self, response):
        # 初始化yield返回数据
        attributes = {'ASIN':'','Title':'','Language':'','Release date':'','Date First Available':'','Run time':'','Producers':'','Directors':'','Writers':'','Actors':'','Media Format':'','Subtitles':'', 'Genres':''}
        
        # 确定ASIN值
        asin_begin_position = 26
        asin_length = 10
        attributes['ASIN'] = response.url[asin_begin_position:asin_begin_position + asin_length]
        
        # 判断类别
        product_type = response.xpath('//*[@id="nav-search-label-id"]/text()').get()

        # 如果都不是 直接return
        if product_type != 'Movies & TV' and product_type != 'Prime Video':
            return 

        # 写入文件 以备不时之需
        path = 'WebPages/'
        # path = '/Volumes/PortableSSD/WebPages'
        filename = f'{path}/{attributes["ASIN"]}.html'
        with open(filename, 'wb') as f:
            f.write(response.body)

        # Movies & TV
        if product_type == 'Movies & TV':
            # 获取标题
            Title = response.xpath('//*[@id="productTitle"]/text()').extract_first().strip()
            attributes['Title'] = Title

            # 获取Product details
            result = response.xpath('//*[@id="detailBullets_feature_div"]/ul/li/span/span/text()').getall()
            result = [r.replace(':','').replace('\u200f','').replace('\u200e','').replace('\t', '').replace('\n', '').replace('\r', '').strip() for r in result]
            
            columns = result[0::2]
            value = result[1::2]

            for i in range(len(columns)):
                for key in attributes.keys():
                    if columns[i] in key:
                        attributes[key] = value[i]

        # Prime Video
        elif product_type == 'Prime Video':
            # 是否为电影
            if '"titleType":"movie"' not in response.xpath('//*[@id="a-page"]/div[2]/script[15]/text()').get():
                return

            # Title = response.xpath('//*[@id="a-page"]/div[2]/div[4]/div/div/div[2]/div[3]/div/h1/text()').get() # 原先代码有误
            Title = response.xpath('//*[@id="a-page"]/div[2]/div[4]/div/div/div[2]/div[2]/div/h1/text()').get()
            if Title == None:
                Title = response.xpath('//*[@id="a-page"]/div[2]/div[4]/div/div/div[2]/div[1]/div/h1/text()').get()
                
            attributes['Title'] = Title

            columns_1 = response.xpath('//*[@id="btf-product-details"]/div/dl/dt/span/text()').getall()
            value_1 = response.xpath('//*[@id="btf-product-details"]/div/dl/dd/*/text()').getall()

            for i in range(len(columns_1)):
                for key in attributes.keys():
                    if columns_1[i] in key:
                        attributes[key] = value_1[i]

            columns_2 = response.xpath('//*[@id="meta-info"]/div/dl/dt/span/text()').getall()
            for i in range(len(columns_2)):
                # 根据columns内容判断
                if columns_2[i] == 'Directors' or columns_2[i] == 'Genres':
                    attributes[columns_2[i]] = response.xpath('//*[@id="meta-info"]/div/dl[' + str(i + 1) + ']/dd/a/text()').get()
                elif columns_2[i] == 'Starring':
                    attributes['Actors'] = str(response.xpath('//*[@id="meta-info"]/div/dl[' + str(i + 1) + ']/dd/a/text()').getall())[1:-2].replace("'",'')
        
        yield attributes
```

首先根据搜索框的类别来判断网页的类型

- 如果是Movies & TV，那么就按普通页面的内容获取标题和内容
- 如果是Prime Video，则再判断该类型是否为movie，是则提取网页内容

此外，每次访问的网页内容也写入到本地，方便后续调用

### 反反爬

亚马逊具有很强的反爬机制，爬取片刻便开启反爬机制，要求输入验证码，大致图片如下

![img](数据仓库项目报告.assets/(null)-20221230114831294.(null))

另外，对于请求过度频繁的ip，亚马逊也会禁止其访问内容

对此，需要进行反反爬措施。一般的方法包括ip池代理＋伪造请求头的方法，还有使用Selenium模拟手动打开网页。Selenium本身是为Web浏览器提供的一个测试工具，为测试的自动化提供了一系列方法，由于其在某种程度上说类似于模拟人进行浏览器的操作，相比单纯的发送ip请求，可以绕过更多限制，减少反爬的可能，因此也有越来越多采用Selenium进行爬虫的方案。此处采用了Selenium作为scrapy的middleware，先用Selenium打开一个浏览器，再打开指定的网页，并作为response返回。

核心代码如下：

```Python
    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        self.driver.get(request.url+"/?language=en_US")
        self.driver.refresh()

        # 检测机器人
        robot_sentence = "Sorry, we just need to make sure you're not a robot. For best results, please make sure your browser is accepting cookies."
        if robot_sentence in self.driver.page_source:
            from lxml import etree
            html = etree.HTML(self.driver.page_source)

            from amazoncaptcha import AmazonCaptcha
            link = html.xpath("/html/body/div/div[1]/div[3]/div/div/form/div[1]/div/div/div[1]/img/@src")[0]
            captcha = AmazonCaptcha.fromlink(link)
            solution = captcha.solve()

            from selenium.webdriver.common.by import By
            input_element = self.driver.find_element(By.ID,"captchacharacters")
            input_element.send_keys(solution)

            button = self.driver.find_element(By.XPATH,"//button")
            button.click()
            time.sleep(3)

        source = self.driver.page_source
        response = HtmlResponse(url = self.driver.current_url, body = source, request = request, encoding = 'utf-8')
        return response
```

爬取过程中发现有两个要点：

- 经过一定的爬取后，发现数据并不完整，经debug发现，网页在加载时不一定能完全加载完，下面的Product details可能不会加载成功，而是变成一段sorry的提示，再刷新后才会正常显示。因此在每次Selenium获取网页后，调用`refresh()`方法再次刷新
- 爬取时有较小概率出现验证码的问题。经查阅，选用了Github上开源的`amazoncaptcha`，它可以根据传入的图片链接获得对应的验证码数字。因此在处理验证码时，先用xpath获取验证码图片的链接，再调用用`amazoncaptcha`得出结果，并用Selenium模拟输入验证码按下按钮，从而解决验证码问题

解决这些问题后，经过一段时间的爬取，获得了网页内容及对应数据

## 合并相同电影

观察电影数据，可以发现很多电影存在极大相似性，主要是在版本（VHS，DVD，Blu-ray）或放映年份等有所区别。为保证数据的质量，对相似的电影进行合并

![img](数据仓库项目报告.assets/(null)-20221230114831572.(null))

主要采用pandas+fuzzywuzzy的方法合并数据

- pandas是一个著名的python的数据分析库，在处理数据方面工具齐全，性能较快
- [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy)是一个匹配字符串的库，采用[Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance)计算字符串直接的相似度

![img](数据仓库项目报告.assets/(null)-20221230114831555.(null))

针对合并相同电影的需求，给出如下方法：

1. 首先去除Title中关于版本的信息（VHS，DVD等），并删除最外边多余的引号
2. 利用Levenshtein Distance计算相似度，选取那些得分高于95的，视为同一电影
3. 在合并电影时
   1. Title选名字最短的
   2. Release Date，Date First Available选最早的
   3. Run time，Producers，Directors，Writers，Actors，Genres一般出现的都不会有不同，选取第一个出现的
   4. Producers，Directors，Writers，Actors使用列表分割
   5. Media Format，Subtitles将所有可能的结果纳入到一个集合中

核心代码见`merge_same_title_and_record_source.ipynb`

另外，考虑到fuzzywuzzy在计算大量距离时耗时较长，因此先将电影名排序，再将每部电影与之后10部电影的相似度进行比较，从而减少了运行的时间

## 合并人名

人名的格式不尽相同，有时相同的人会有不同的名字，情况如下：

- 人名颠倒，或者缺少空格

![img](数据仓库项目报告.assets/(null)-20221230114831499.(null))

- 大小写不同

<img src="数据仓库项目报告.assets/(null)-20221230114831823.(null)" alt="img" style="zoom:50%;" />

- middle name缩写

<img src="数据仓库项目报告.assets/(null)-20221230114831534.(null)" alt="img" style="zoom:67%;" />

经过测试，仍采用fuzzywuzzy库进行相似度比较

1. 首先读取电影信息，抽离出所有导演，编剧和演员的名字，合并到names数组中
2. 对其进行去重，降序排序
   1. 这样是为了保证更加规范的小写字母的名字在前，以确保替换时以先遍历到的规范名字在前
3. 遍历数组，计算首字母相同的名字之间的相似度，选取得分高于95的进行替换

（具体代码见`merge_similar_names.ipynb`）

## 上映日期

部分电影缺少上映日期。对此，从之前提取的评论数据出发，选取对应ASIN值的评论中的最早时间，将其定义为上映日期

```Python
import math
i = 0
for index, row in movies_info.iterrows():
    if pd.isnull(row['Release date']):
        ID = row['ASIN']
        print(ID, end = ' ')
        earliest_time = comments.loc[comments['productId']==ID,'time'].min()
        print(earliest_time)
        if not math.isnan(earliest_time):
            earliest_time_str = time.strftime("%B %d, %Y", time.localtime(earliest_time))
#             movies_info.loc[movies_info['ASIN'] == ID, 'Release date'] = earliest_time_str
            row['Release date'] = earliest_time_str
            i += 1
            if i % 1000 == 0:
                print(i)
#             print(row)
```

## 数据血缘

在合并相同电影的同时，建立了一个新的名为`source_asin`的DataFrame，列的信息与电影信息相同，行数也保持一致，而每个单元格的信息则是对应电影信息表中的数据的来源ASIN值。通过这种方式，可以找到每个信息的来源

![img](数据仓库项目报告.assets/(null)-20221230114831884.(null))

![img](数据仓库项目报告.assets/(null)-20221230114831722.(null))

## 后续处理

基于项目的需求和爬取存储的本地网页，以pandas为主，对数据进行了再次清洗，包括

- 格式的修改
- 添加index
- 基于review计算score和edition
- 根据review内容中是否包含'movie'和'film'再次筛选电影
- 删除review中的text内容（过于庞大但没有用处）
- 删除actors中的various
- 删除多余的引号
- 整理为适合导入数据库的格式

具体处理见`pre-process-*.ipynb`，`data-prepare-*.ipynb`

# 存储分析

对每种存储方式结合本项目说明各自适用于处理什么查询，针对本项目在存储优化中做了什么工作，优化前后的比较结果是怎样的

## 存储方式分析

### 关系型适用查询

关系型数据库存放在特定结构的表中，必须定义好表和字段结构后才能添加数据，需要增加外部关联数据的话，规范化做法是在原表中增加一个外键，关联外部数据表。结构化的存储方式较为严谨，通过建立表结构能有效规范和划分数据存储的类别。同时，复杂查询可以用SQL语句方便地在多个表之间做非常复杂的数据查询。它适用于对数据量不是特别大、对安全性要求较高、数据格式单一的数据。此外，关系型数据库还支持一些特性，如索引支持可以提高查询性能，事务支持对于安全性能很高的数据访问要求得以实现。

在本项目中，我们对物理模型进行了反规范化，即使用星型模型，包含了大量冗余信息。使得在进行某些特殊查询时，不再需要对表进行join操作，用空间换取了时间，如在Act表（电影—演员关系表）、Direct表（电影、导演关系表）中添加了电影标题字段，这样在查询演员所演电影或导演所导电影时就不必联合查询电影表；同时，对于上映时间表，我们抽离出年、月、日、季度和星期五个变量将其存储为单独字段，在根据上映时间查询时便不再需要对综合属性date进行拆分，从而提高查询速率。

### 分布式适用查询

分布式数据查询的使用场景如下：

- 具有扩容需求。分布式的数据存储在不同的物理设备中，需要增加数据时，可以通过添加设备来横向拓展，解决容量问题
- 处理时效性不高的大规模数据时。分布式查询的执行延迟比较高，因此不适用于实时性要求高的场合。但当数据规模庞大时，从之前的Hadoop的MapReduce算法，到最近的Spark将数据在内存中运算，并行查询子查询，分布式计算的速度优势越发明显。此外，基于Hadoop的Hive提供了类似SQL的HQL语法，也使得分布式查询更加容易上手
- 容错能力要求高。分布式的数据分布在各处，因此容错能力更强，风险更小。

### 图数据库适用查询

neo4j对具有以下需求的场景的应用较为合适

- neo4j是一个面向**网络**的**数据库**，是一个嵌入式的、基于磁盘的、具备完全的事务特性的Java持久化引擎，它将结构化数据存储在网络上而不是表中。图是一个灵活的数据结构，可以应用更加敏捷和快速的开发模式。
- neo4j很容易表示连接的数据，检索/遍历/导航更多的连接数据是非常容易和快速的
- neo4j非常容易地表示半结构化数据，使用简单而强大的数据模型
- neo4j不需要复杂的连接来检索连接的/相关的数据，因为其很容易检索它的相邻节点或关系细节没有连接或索引

## 存储优化

### 关系型

1. 字段属性优化

- 针对每个字段的存储类型，我们统计该字段数据的范围后，将诸如INT优化为TINYINT，VARCHAR的长度只分配真正需要的空间
- 将review中一些不会用到的字段删去，减少表的宽度来提高join速度
- 使用时间戳代替datetime类型，在使用时再转换为具体的时间
- 表字段避免NULL值出现，NULL值很难查询优化且占用额外的索引空间，可以使用默认数字0代替NULL

2. 建立索引

索引并不是越多越好，要根据查询有针对性的创建，从而提高查询速度，我们对索引的处理如下：

- 考虑在WHERE和ORDER BY命令所在字段，以及一些经常用来筛选和查询的字段建立索引。如根据演员导演来查询电影名，当输入名字后需要去遍历演员名或导演名字段，在这里建立索引可以显著减少扫描次数。我们使用EXPLAIN命令来查看是否用了索引还是全表扫描，考虑如下查询：

  ```SQL
    select Act.movie_title from dw.Act 
    right join Actor on Act.actor_id = Actor.actor_id 
    where Actor.name like 'David%'
  ```

  不使用索引时，扫描的rows到达了109904行，即整个表所有行都被遍历。

  ![img](数据仓库项目报告.assets/(null)-20221230114831745.(null))

  使用索引时，扫描的rows显著降低，减少了100倍左右，并且访问类型type也从ALL变为range，即只检索给定范围的行。

  ![img](数据仓库项目报告.assets/(null)-20221230114831951.(null))

  同理，我们在导演姓名、电影名称等地方都建立了字符串索引。特别地，对于导演演员和电影名，我们提供了搜索建议查询，因此对数据库的访问较为频繁且速度要求较高，因此即使对字符串建立索引的空间占比较大，总体上依然能提高效率。另外，对于演员和导演的关系表，我们对他们的id建立了索引，将原本4-5秒提升到0.1s的级别，大大提升关系型查询的效率。

- 考虑到time表的字段繁多，包含年、月、日、星期和季度五个，且很多情况都是多个字段联合查询，因此我们考虑建立组合索引，在创建索引的前后，可以发现扫描的行也显著减少，并且访问类型也从ALL变成ref，经过测试，所需时间有较大提升。

  ```SQL
    explain
    select Movie.title from dw.Movie 
    left join ReleaseDate RD on Movie.movie_id = RD.movie_id 
    where year = 2005 and month = 5
  ```

  ![img](数据仓库项目报告.assets/(null)-20221230114831728.(null))

  ![img](数据仓库项目报告.assets/(null)-20221230114831747.(null))

3. 字段冗余

在进行电影的筛选时，筛选条件字段大概率不会出现在电影表中，因为我们为了使电影事实表尽量小，我们通过外键将很多属性放在了纬度表中，这样导致在查询时会涉及很多表的join操作。

因此我们通过在纬度表中建立字段冗余来减少表和表之间的join。如在演员导演和电影的关系表中，添加冗余的movie_title字段，从而在根据演员或导演查询电影名时不再需要join电影表，再如将电影的评分直接存入电影表，因为评分本身非常轻量级，且和电影是一对一的关系，减少根据评分查询电影时的join。

冗余字段的选择需要考虑实际，在演员导演合作关系表中，我们共存储了100w多条两两合作关系表，此时若再次添加合作电影字段所带来的冗余，远远大于join两表的开销，因此此处不作冗余字段的设置。

4. 建立视图

为了减少综合查询时所涉及到的表的join操作，我们把与movie表有一对一关系的表join到一起建立视图；而对于一对多、多对多等关系的表，我们也分别单独建立视图，如演员表、电影表和演员-电影关系表。

1. 综合查询视图

在该视图中，我们将与movie一对一关系的字段全部join进来，减少在综合查询时的join操作，并且也尽可能减少了冗余字段。

  ```SQL
  create definer = liuchang@`%` view movie_date_genre as
  select `dw`.`Movie`.`title`    AS `title`,
  `dw`.`Movie`.`movie_id` AS `movie_id`,
  `dw`.`Movie`.`score`    AS `score`,
  `dw`.`Movie`.`edition`  AS `edition`,
  `RD`.`year`             AS `year`,
  `RD`.`month`            AS `month`,
  `RD`.`day`              AS `day`,
  `RD`.`season`           AS `season`,
  `RD`.`weekday`          AS `weekday`,
  `RD`.`date`             AS `date`,
  `G`.`genre_name`        AS `genre_name`
  from ((`dw`.`Movie` left join `dw`.`ReleaseDate` `RD` on ((`dw`.`Movie`.`movie_id` = `RD`.`movie_id`)))
  left join `dw`.`Genre` `G` on ((`dw`.`Movie`.`movie_id` = `G`.`movie_id`)));
  ```

2. 演员（导演）查询视图
	
	由于演员和电影是多对多的关系，为了减少演员姓名的冗余，我们将演员单独存储为一张表，因此在演员和电影之间必须建立关系表。但是在涉及查询电影的演员时便需要进行两次join操作，较为复杂。在这里我们构建电影id和演员姓名的视图，可以直接使用电影id查询演员名称，从而加快查询速度。

### 分布式

- 数据结构
  - 建立了与MySQL关系型数据库相似的数据表，通过冗余数据换取查询效率
- 建立视图
  - 建立了与MySQL关系型数据库相似的视图，以减少join操作

### 图数据库

1. 关系建立的优化

通过对合作关系的建立，而不再遍历所有的act或者direct关系进行计数并且排序，这样既限制了搜索范围，又减慢了搜索时间。在合作关系的建立中，往关系里添加合作次数属性，省去了计数的时间。由于双向关系的引入，可以通过输入起点的方法很快的找到与该起点有关的所有关系，并且由于加入了索引，所以搜索节点的速度获得很大的提升，而找到起始节点与其第一层关系只需要极短的时间

<img src="数据仓库项目报告.assets/(null)-20221230114832129.(null)" alt="img" style="zoom:50%;" />

2. 建立索引

对节点建立索引表可以帮助我们很快的通过属性来找到相对应的节点，而通过输入起始点的方法又很容易的可以获得对应的关系以及具有关系的节点，而这样的空间的牺牲来换取查找的速度是值得的

<img src="数据仓库项目报告.assets/(null)-20221230114832055.(null)" alt="img" style="zoom:50%;" />

3. 查询方式的优化

使用了py2neo所提供的relationshipmatch和nodematch来进行查询，虽然舍弃了使用Cypher的灵活性，但代码风格更为简约，在通过nodematch进行有起点的查询方法，而非遍历所有关系进行查询，极大的缩短了查询时间，虽然让存储空间大大提高，但是查询速度的提升是较为明显的。

```Python
// 以演员之间的合作关系查询为例
node_matcher = NodeMatcher(graph)
relationship_matcher = RelationshipMatcher(graph)
node1 = node_matcher.match('actor').where(name=inname).first()
filnum = "_.times>=" + str(times)
relationship = list(relationship_matcher.match([node1], r_type='actope').where(filnum).all())
```

# 溯源查询

- 在过程中，我们找出了多少非电影的数据？

我们一共爬取了233131条产品数据，其中筛选了80660条非电影数据，获得了152471条电影数据，最终合并得到83921部电影的数据

- 哈利波特相关问题

共找到25部哈利波特系列电影，相关ASIN见第六列（显示不完全）

![img](数据仓库项目报告.assets/(null)-20221230114832104.(null))

哈利波特第一部（Harry Potter and the Sorcerer's Stone）合并了7个网页

![img](数据仓库项目报告.assets/(null)-20221230114832030.(null))

# 数据质量

## 如何保证数据质量

- 数据爬取
  - 通过爬虫获取数据时，提前针对反爬措施安排对策（模拟输入验证码，更换ip地址，更换cookie等）
  - 爬虫的网页出现问题（如404无法找到，503被反爬）时，对问题数据进行标记，之后进行分析与重新爬取
  - 提升信息规范性。包括去除信息中的冗余字符（换行，引号，空格），统一大小写，合并相似字段等
  - 注意特殊符号。有些电影带有的'/'，'&'等符号带有特殊含义，可能导致匹配识别错误，导致返回结果出错
- 数据清洗
  - 通过互补信息提升信息质量。Amazon电影数据总体记录数量较多，但是部分电影数据缺失严重。对此，可以通过评论集信息、imdb信息、豆瓣信息等进行信息互补。例如在本次项目中，我们将评论集中的最早评论时间作为电影的上映时间，减少了信息的缺失

## 哪些情况会影响数据质量

1. 可能存在数据库表结构、数据库约束条件、数据校验规则的设计开发不合理，造成数据录入无法校验或校验不当，引起数据重复、不完整、不准确等数据模型设计的质量问题。
2. 数据是通过爬虫从网页上采集的，在网页上这些数据可能就存在重复、不完整、不准确等问题，而采集过程有没有对存在这些问题的数据做清洗处理，数据源存在数据质量问题。
3. 数据采集过程质量问题，由于数据采集的网站Amazon的反爬策略较为完善，可能由于采集点、采集频率、采集内容、映射关系等采集参数和流程设置的不正确，数据采集接口效率低，导致的数据采集失败、数据丢失、数据映射和转换失败等数据采集过程质量问题。
4. 可能存在数据接口本身存在问题、数据接口参数配置错误、网络不可靠等都会造成数据传输过程中的发生数据质量问题。
5. 可能存在数据清洗规则、数据转换规则、数据装载规则配置等数据装载过程的问题。
6. 可能存在数据存储设计不合理，数据的存储能力有限，人为后台调整数据，引起的数据丢失、数据无效、数据失真、记录重复等数据存储的质量问题。

# 数据血缘

数据血缘的使用场景

1. 保证报告数据的完整性

开发人员可以通过检查数据血缘链中的每个节点，追溯异常数据元素的血缘，以确认数据的计算处理方式，并分析与该异常数据有交互的业务用户行为，实现异常数据元素的排查，确认数据变更影响的下游数据对象，保证数据的完整性。

2. 提升调度性能 

通过收集调度任务的开始结束时间，了解关键任务ETL链路的时间瓶颈，再根据JOB任务的执行情况，定位到性能瓶颈通过调整任务的优先级，保证任务的资源提供，提升整条ETL链路的执行效率。 

3. 追踪个人信息，控制传播范围

数据血缘可以将追踪个人信息扩展到数据报告层和数据库层，如果将报表中的特定数据元素进行标识，则能够在涉及该数据元素的所有血缘路径中找到标识元素所在列，并使用数据血缘工具控制数据传播范围。

4. 给表和字段打标签 

通过血缘分析，对整条链路打上标签，比如业务数据、广告、订单之类，也可以打上优先级重要程度的标签。

# 小组成员及得分权重

| 学号 | 姓名                                          | 权重 |
| ---- | --------------------------------------------- | ---- |
| 1    | [Baokker](https://github.com/Baokker)         | 100% |
| 2    | [Gxyrious](https://github.com/Gxyrious)       | 100% |
| 3    | [DEM1TASSE](https://github.com/DEM1TASSE)     | 100% |
| 4    | [ChillyGWind](https://github.com/ChillyGWind) | 100% |
